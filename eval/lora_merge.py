import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ====== ç”¨æˆ·è®¾ç½®åŒºåŸŸ ======
base_model_path = "/p/work2/yuxuanj1/hf_models/DeepSeek-R1-Distill-Qwen-7B"
adapter_path = "/p/work2/yuxuanj1/ftmodels/qwen-7b-limo-3ep/lora-sft"
output_path = "/p/work2/yuxuanj1/merged_models/qwen-7b-limo-3ep"
# ==========================

os.makedirs(output_path, exist_ok=True)

print("ğŸ”„ Loading base model...")
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    trust_remote_code=True,
    torch_dtype="auto"
)

print("ğŸ”Œ Loading LoRA adapter...")
model = PeftModel.from_pretrained(base_model, adapter_path)

print("ğŸ§¬ Merging adapter into base model...")
model = model.merge_and_unload()

print("ğŸ’¾ Saving merged model to:", output_path)
model.save_pretrained(output_path)

# å¯é€‰ï¼šä¹Ÿä¿å­˜ tokenizerï¼ˆå»ºè®®ä¸€èµ·ç”¨ï¼‰
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
tokenizer.save_pretrained(output_path)

print("âœ… Merge complete. Model is ready to be used with vLLM.")
