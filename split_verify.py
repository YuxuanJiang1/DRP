import pandas as pd
import re
import openai
from tqdm import tqdm
import tiktoken

# è®¾ç½® OpenAI API Key
openai.api_key = ('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')

# è¯»å–åŸå§‹ GPT-4o è¾“å‡ºæ•°æ®
df = pd.read_csv("R1-Qwen-7B/7B_output_on_train.csv").head(100)

# ---------- ç¬¬ä¸€é˜¶æ®µï¼šæå– <think> æ¨ç†è·¯å¾„å¹¶åˆ†æ­¥ ---------- #

step_segmentation_prompt = (
    """You will be provided with a full reasoning path. 
Your task is to break it into a sequence of clear, non-overlapping steps, where each step corresponds to exactly one atomic mathematical skill (e.g., addition, subtraction, applying a formula, interpreting a quantity, simplifying, checking a condition).
Do not skip or rewrite anything. Cover every part of the original text â€” all explanations, calculations, and equations â€” step by step, without omission.
Use the format: Step n: {segment of original text}\nSkill: {skill used}. Only split and label steps. Don't skip anything, like the last sentence: the final answer is xx, you can write down:step n:the answer is xx. skill: clarify answer.
Only segment and label the steps â€” do not solve or modify original content."""
)

def extract_and_split_think(response):
    match = re.search(r"<think>(.*?)</think>", response, re.DOTALL)
    reasoning_text = match.group(1).strip() if match else response

    messages = [
        {"role": "system", "content": step_segmentation_prompt},
        {"role": "user", "content": f"<think>\n{reasoning_text}\n</think>"}
    ]

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=messages,
            temperature=0,
            max_tokens=1024
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print("âŒ Step extraction error:", e)
        return "[error]"

print("ğŸš€ ç¬¬ä¸€æ­¥ï¼šæå– reasoning steps ä¸­...")
df["reasoning_steps"] = [extract_and_split_think(resp) for resp in tqdm(df["qwen_full_response"])]

# ä¿å­˜æ­¥éª¤æ‹†åˆ†ä¸­é—´ç»“æœ
df.to_csv("baselines/gpt4o_output_with_steps.csv", index=False, encoding="utf-8")

# ---------- ç¬¬äºŒé˜¶æ®µï¼šå‹ç¼©æ­¥éª¤ + ç”Ÿæˆæµç•…æ¨ç† ---------- #

compression_prompt = (
    "You are an expert in mathematical reasoning compression.\n\n"
    "Given a list of reasoning steps, each labeled with the skill used, your task is to evaluate whether and how each step should be simplified to improve efficiency without losing necessary logic.\n\n"
    "You have four actions to choose from for each step:\n\n"
    "1. KEEP: The step is necessary and already concise. Keep it unchanged.\n"
    "2. DELETE: The step is unnecessary and should be removed entirely.\n"
    "3. SINGLE-STEP COMPRESS: The step is necessary but verbose; rewrite it in a more concise way.\n"
    "4. MULTI-STEP COMPRESS: The step can be merged with neighboring steps; write a combined, cleaner version.\n\n"
    "For the last step, if it's to clarify the final answer,like 'the answer is xx', keep it.\n"
    "After youâ€™ve completed the step evaluations and transformations, synthesize them into a single, coherent reasoning path that maintains a fluent tone and matches the original speakerâ€™s style. At the end ,please make sure there is a clarification of the answer, with 'The answer is'.\n\n"
    "Output the integrated compressed reasoning starting with ##."
)

def compress_and_verify(question, reasoning_steps_text):
    user_input = f"Question:\n{question}\n\nReasoning Steps:\n{reasoning_steps_text}"

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": compression_prompt},
                {"role": "user", "content": user_input}
            ],
            temperature=0,
            max_tokens=1024
        )
        full_output = response.choices[0].message.content.strip()
        match = re.search(r"##\s*(.*)", full_output, re.DOTALL)
        compressed_response = match.group(1).strip() if match else "[no ## result found]"
        return full_output, compressed_response
    except Exception as e:
        print("âŒ Compression error:", e)
        return "[error]", "[error]"

print("ğŸš€ ç¬¬äºŒæ­¥ï¼šéªŒè¯å¹¶å‹ç¼© reasoning steps...")
verified_outputs = []
compressed_outputs = []

for _, row in tqdm(df.iterrows(), total=len(df)):
    full, compressed = compress_and_verify(row["question"], row["reasoning_steps"])
    verified_outputs.append(full)
    compressed_outputs.append(compressed)

df["verified_response"] = verified_outputs
df["compressed_response"] = compressed_outputs

# ä¿å­˜æœ€ç»ˆç»“æœ
output_path = "baselines/gpt4o_verified_compressed.csv"
df[["question", "verified_response", "compressed_response"]].to_csv(output_path, index=False, encoding="utf-8")


def analyze_token_usage(df, output_csv_path):
    print("\nğŸ“Š æ­£åœ¨åˆ†æ token æ•°é‡...")

    # åˆå§‹åŒ– tokenizer
    enc = tiktoken.encoding_for_model("gpt-4o")

    # å®šä¹‰è®¡æ•°å‡½æ•°
    def count_tokens(text):
        return len(enc.encode(text)) if isinstance(text, str) else 0

    # é€è¡Œè®¡ç®—
    df["verified_token_count"] = df["verified_response"].apply(count_tokens)
    df["compressed_token_count"] = df["compressed_response"].apply(count_tokens)

    # æ±‡æ€»ä¿¡æ¯
    total_verified = df["verified_token_count"].sum()
    total_compressed = df["compressed_token_count"].sum()
    avg_verified = df["verified_token_count"].mean()
    avg_compressed = df["compressed_token_count"].mean()

    # æ‰“å°ç»“æœ
    print("ğŸ§® Tokenç»Ÿè®¡ç»“æœï¼š")
    print(f"âœ… åŸå§‹ verified_response æ€»tokenæ•°: {total_verified}ï¼Œå¹³å‡æ¯æ¡: {avg_verified:.2f}")
    print(f"âœ… å‹ç¼© compressed_response æ€»tokenæ•°: {total_compressed}ï¼Œå¹³å‡æ¯æ¡: {avg_compressed:.2f}")
    print(f"ğŸ¯ èŠ‚çœæ¯”ä¾‹: {(1 - total_compressed / total_verified) * 100:.2f}%")

    # ä¿å­˜åŒ…å« token æ•°çš„æ–° CSV
    output_with_tokens = output_csv_path.replace(".csv", "_token_count.csv")
    df.to_csv(output_with_tokens, index=False, encoding="utf-8")
    print(f"ğŸ“ ä¿å­˜å« token ç»Ÿè®¡çš„æ–‡ä»¶è‡³ï¼š{output_with_tokens}")

# è¿è¡Œ token åˆ†æï¼ˆæ·»åŠ åœ¨ä¸»æµç¨‹æœ«å°¾ï¼‰
analyze_token_usage(df, output_path)

print(f"âœ… æ‰€æœ‰å¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜è‡³ï¼š{output_path}")
